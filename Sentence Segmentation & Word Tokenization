pip install nltk spacy
python -m spacy download en_core_web_sm

# Sentence Segmentation  (part 1)
doc = nlp(u"I Love Coding. Geeks for Geeks helped me in this regard very much. I Love Geeks for Geeks.")
for sent in doc.sents:
  print(sent)


# Word Tokenization  (part 2)
text = "A good traveler has no fixed plans and is not intent on arriving"
sentences = nltk.sent_tokenize(text)
for sentence in sentences:
    words = nltk.word_tokenize(sentence)
    print(words)
